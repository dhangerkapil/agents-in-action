{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f9c4bec",
   "metadata": {},
   "source": [
    "# Azure AI Chat Client Observability\n",
    "\n",
    "This notebook demonstrates **built-in telemetry** in Azure AI using the `AzureAIAgentClient` with streaming responses. It shows how to leverage Azure AI's telemetry capabilities to monitor chat interactions, tool calls, and code execution.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "- ‚úÖ How to use **streaming responses** with observability\n",
    "- ‚úÖ How to track **multiple tools** (weather function + code interpreter)\n",
    "- ‚úÖ How to monitor **token usage** across operations\n",
    "- ‚úÖ How to use **trace IDs** for debugging in Application Insights\n",
    "- ‚úÖ How to see **Azure AI-specific operations** in traces\n",
    "\n",
    "## Key Differences from Agent Observability\n",
    "\n",
    "This sample uses the **chat client** directly instead of creating a `ChatAgent`. This approach:\n",
    "- Shows lower-level telemetry integration\n",
    "- Demonstrates Azure AI-specific operations (like `create_agent`)\n",
    "- Uses `get_streaming_response()` method\n",
    "- Tracks multiple tool types simultaneously\n",
    "\n",
    "> **Note**: You must have an **Application Insights** instance attached to your Azure AI project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e91b2e3",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook:\n",
    "\n",
    "1. **Application Insights** connected to your Azure AI Foundry project\n",
    "2. **Azure CLI** authenticated (`az login --use-device-code`)\n",
    "3. **Environment variables** in `../.env`:\n",
    "   - `AZURE_AI_PROJECT_ENDPOINT`\n",
    "4. **Packages installed**:\n",
    "   ```bash\n",
    "   pip install agent-framework agent-framework-azure-ai python-dotenv azure-identity\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28407d5",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "Import libraries for chat client, tools, and observability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft. All rights reserved.\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "from random import randint\n",
    "from typing import Annotated\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import Field\n",
    "\n",
    "from agent_framework import HostedCodeInterpreterTool\n",
    "from agent_framework.azure import AzureAIAgentClient\n",
    "from agent_framework.observability import get_tracer\n",
    "from azure.ai.projects.aio import AIProjectClient\n",
    "from azure.identity.aio import AzureCliCredential\n",
    "from opentelemetry.trace import SpanKind\n",
    "from opentelemetry.trace.span import format_trace_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83a8d9b",
   "metadata": {},
   "source": [
    "## Environment Configuration\n",
    "\n",
    "Load the Azure AI project endpoint from environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e65407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from parent directory\n",
    "load_dotenv('../.env')\n",
    "\n",
    "# Validate required environment variable\n",
    "required_vars = ['AZURE_AI_PROJECT_ENDPOINT']\n",
    "missing = [var for var in required_vars if not os.getenv(var)]\n",
    "\n",
    "if missing:\n",
    "    raise RuntimeError(f'Missing required environment variables: {missing}')\n",
    "\n",
    "endpoint = os.getenv('AZURE_AI_PROJECT_ENDPOINT')\n",
    "print(f'‚úÖ AZURE_AI_PROJECT_ENDPOINT: {endpoint}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdd613a",
   "metadata": {},
   "source": [
    "## Define Custom Function Tool\n",
    "\n",
    "Create a weather function that will be traced automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b11ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSI color codes for better console output\n",
    "BLUE = \"\\x1b[34m\"\n",
    "RESET = \"\\x1b[0m\"\n",
    "\n",
    "async def get_weather(\n",
    "    location: Annotated[str, Field(description=\"The location to get the weather for.\")],\n",
    ") -> str:\n",
    "    \"\"\"Get the weather for a given location.\"\"\"\n",
    "    # Simulate a network call delay\n",
    "    await asyncio.sleep(randint(0, 10) / 10.0)\n",
    "    \n",
    "    conditions = [\"sunny\", \"cloudy\", \"rainy\", \"stormy\"]\n",
    "    temperature = randint(10, 30)\n",
    "    weather_condition = conditions[randint(0, 3)]\n",
    "    \n",
    "    return f\"The weather in {location} is {weather_condition} with a high of {temperature}¬∞C.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a30dd9",
   "metadata": {},
   "source": [
    "## Understanding Chat Client Telemetry\n",
    "\n",
    "### What Gets Traced\n",
    "\n",
    "When using `AzureAIAgentClient.get_streaming_response()`, telemetry captures:\n",
    "\n",
    "1. **Azure AI Operations**:\n",
    "   - `create_agent`: Agent creation in Azure AI Foundry\n",
    "   - `create_thread`: Thread initialization\n",
    "   - `create_run`: Run creation for processing\n",
    "\n",
    "2. **Chat Operations**:\n",
    "   - Model invocations with request/response details\n",
    "   - Token usage (input/output)\n",
    "   - Response streaming chunks\n",
    "\n",
    "3. **Tool Executions**:\n",
    "   - Custom function calls (`get_weather`)\n",
    "   - Code interpreter executions\n",
    "   - Tool arguments and results (if sensitive data enabled)\n",
    "\n",
    "### Multiple Tools Support\n",
    "\n",
    "This example demonstrates using **two types of tools**:\n",
    "- **Custom function**: `get_weather` for weather queries\n",
    "- **Hosted code interpreter**: For Python code execution\n",
    "\n",
    "Both are automatically traced in Application Insights!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061563b4",
   "metadata": {},
   "source": [
    "## Main Demo: Streaming Chat with Multiple Tools\n",
    "\n",
    "This demo processes multiple questions using streaming responses and different tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd467c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_chat_with_observability():\n",
    "    \"\"\"\n",
    "    Run Azure AI chat client with observability enabled.\n",
    "    \n",
    "    This demonstrates:\n",
    "    - Streaming responses with telemetry\n",
    "    - Multiple tool types (function + code interpreter)\n",
    "    - Azure AI-specific operation tracing\n",
    "    - Token usage tracking\n",
    "    \"\"\"\n",
    "    # Questions demonstrating different capabilities\n",
    "    questions = [\n",
    "        \"What's the weather in Amsterdam and in Paris?\",\n",
    "        \"Why is the sky blue?\",\n",
    "        \"Tell me about AI.\",\n",
    "        \"Can you write a python function that adds two numbers? and use it to add 8483 and 5692?\",\n",
    "    ]\n",
    "    \n",
    "    # Create async context managers for Azure resources\n",
    "    async with (\n",
    "        AzureCliCredential() as credential,\n",
    "        AIProjectClient(endpoint=endpoint, credential=credential) as project,\n",
    "        AzureAIAgentClient(project_client=project) as client,\n",
    "    ):\n",
    "        # Enable observability - configures telemetry to Application Insights\n",
    "        print(f\"{BLUE}üîß Setting up Azure AI observability...{RESET}\")\n",
    "        await client.setup_azure_ai_observability()\n",
    "        print(f\"{BLUE}‚úÖ Observability configured{RESET}\\n\")\n",
    "        \n",
    "        # Create a custom span to group all operations\n",
    "        with get_tracer().start_as_current_span(\n",
    "            name=\"Foundry Telemetry from Agent Framework\",\n",
    "            kind=SpanKind.CLIENT\n",
    "        ) as current_span:\n",
    "            # Display trace ID for Application Insights lookup\n",
    "            trace_id = format_trace_id(current_span.get_span_context().trace_id)\n",
    "            print(f\"{BLUE}\" + \"=\"*70 + f\"{RESET}\")\n",
    "            print(f\"{BLUE}üîç Trace ID: {trace_id}{RESET}\")\n",
    "            print(f\"{BLUE}   Use this to find traces in Application Insights{RESET}\")\n",
    "            print(f\"{BLUE}\" + \"=\"*70 + f\"{RESET}\\n\")\n",
    "            \n",
    "            # Process each question with streaming response\n",
    "            for question in questions:\n",
    "                print(f\"{BLUE}üë§ User: {question}{RESET}\")\n",
    "                print(f\"{BLUE}ü§ñ Assistant: {RESET}\", end=\"\")\n",
    "                \n",
    "                # Stream response chunks\n",
    "                # Telemetry automatically captures:\n",
    "                # - Agent creation (if needed)\n",
    "                # - Tool calls (weather function or code interpreter)\n",
    "                # - Model interactions\n",
    "                # - Token usage\n",
    "                async for chunk in client.get_streaming_response(\n",
    "                    question,\n",
    "                    tools=[get_weather, HostedCodeInterpreterTool()]\n",
    "                ):\n",
    "                    if str(chunk):\n",
    "                        print(f\"{BLUE}{str(chunk)}{RESET}\", end=\"\", flush=True)\n",
    "                \n",
    "                print(f\"{BLUE}{RESET}\\n\")  # New line after response\n",
    "            \n",
    "            print(f\"{BLUE}\" + \"=\"*70 + f\"{RESET}\")\n",
    "            print(f\"{BLUE}‚úÖ Demo completed!{RESET}\")\n",
    "            print(f\"{BLUE}   Check Application Insights for trace ID: {trace_id}{RESET}\")\n",
    "            print(f\"{BLUE}\" + \"=\"*70 + f\"{RESET}\")\n",
    "\n",
    "# Run the demo\n",
    "await run_chat_with_observability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd877e0",
   "metadata": {},
   "source": [
    "## Understanding the Telemetry Output\n",
    "\n",
    "### Spans You'll See in Application Insights\n",
    "\n",
    "1. **Top-level span**: `Foundry Telemetry from Agent Framework`\n",
    "   - Groups all operations under one trace ID\n",
    "   - Duration of the entire conversation\n",
    "\n",
    "2. **Azure AI Operations**:\n",
    "   - `create_agent`: First-time agent creation\n",
    "   - `create_thread`: Thread initialization (once per session)\n",
    "   - `create_run`: Each question creates a new run\n",
    "\n",
    "3. **Chat Spans**:\n",
    "   - Model name and system (e.g., \"gpt-4o\", \"openai\")\n",
    "   - Request and response details\n",
    "   - Token usage per interaction\n",
    "\n",
    "4. **Tool Execution Spans**:\n",
    "   - `execute_tool get_weather`: Weather function calls\n",
    "   - `execute_tool code_interpreter`: Python code execution\n",
    "   - Function duration and results\n",
    "\n",
    "### Questions and Tool Usage\n",
    "\n",
    "The demo asks 4 different questions that demonstrate:\n",
    "\n",
    "1. **Weather query**: Triggers `get_weather` function (custom tool)\n",
    "2. **Knowledge question**: Uses only the LLM, no tools\n",
    "3. **General question**: Direct LLM response\n",
    "4. **Code execution**: Triggers `HostedCodeInterpreterTool` for Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5b3d91",
   "metadata": {},
   "source": [
    "## Comparing with Agent Observability\n",
    "\n",
    "### Chat Client Approach (This Notebook)\n",
    "```python\n",
    "async for chunk in client.get_streaming_response(\n",
    "    question,\n",
    "    tools=[get_weather, HostedCodeInterpreterTool()]\n",
    "):\n",
    "    print(chunk, end=\"\")\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Lower-level access to Azure AI operations\n",
    "- See internal Azure AI telemetry (create_agent, create_thread)\n",
    "- Direct streaming from the client\n",
    "\n",
    "### ChatAgent Approach (Previous Notebook)\n",
    "```python\n",
    "agent = ChatAgent(\n",
    "    chat_client=client,\n",
    "    tools=get_weather,\n",
    "    name=\"WeatherAgent\"\n",
    ")\n",
    "async for update in agent.run_stream(question, thread=thread):\n",
    "    print(update.text, end=\"\")\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Higher-level abstraction\n",
    "- Thread management built-in\n",
    "- Agent-level telemetry spans\n",
    "\n",
    "### When to Use Each\n",
    "\n",
    "- **Chat Client**: When you need fine-grained control and want to see Azure AI internals\n",
    "- **ChatAgent**: When you want managed thread state and agent-level abstractions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce243a4",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### View Traces in Azure Portal\n",
    "\n",
    "1. Copy the **Trace ID** from the output above\n",
    "2. Go to **Azure Portal** ‚Üí Your AI Foundry Project\n",
    "3. Navigate to **Application Insights**\n",
    "4. Click **Transaction Search** or **Logs**\n",
    "5. Search for the trace ID\n",
    "6. View the complete trace timeline:\n",
    "   - All spans in chronological order\n",
    "   - Token usage per operation\n",
    "   - Tool execution details\n",
    "   - Performance metrics\n",
    "\n",
    "### Enable Sensitive Data (Development Only)\n",
    "\n",
    "To see prompts, responses, and function arguments in traces:\n",
    "\n",
    "```python\n",
    "from agent_framework.observability import setup_observability\n",
    "setup_observability(enable_sensitive_data=True)\n",
    "```\n",
    "\n",
    "Or set environment variable:\n",
    "```bash\n",
    "ENABLE_SENSITIVE_DATA=true\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è **Warning**: Only enable in development environments!\n",
    "\n",
    "### Explore More\n",
    "\n",
    "- **Custom Metrics**: Add custom counters with `get_meter()`\n",
    "- **Custom Spans**: Create your own spans with `get_tracer()`\n",
    "- **Workflow Telemetry**: See the next notebook for workflow observability\n",
    "- **Aspire Dashboard**: Try local visualization during development\n",
    "\n",
    "### Related Documentation\n",
    "\n",
    "- üìñ [Agent Observability](https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/agent-observability?pivots=programming-language-python)\n",
    "- üìñ [Application Insights](https://learn.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview)\n",
    "- üìñ [OpenTelemetry Python](https://opentelemetry.io/docs/instrumentation/python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
