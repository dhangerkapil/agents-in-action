{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8156a0d",
   "metadata": {},
   "source": [
    "# Suspend and Resume Conversation Threads\n",
    "\n",
    "This notebook demonstrates how to **suspend and resume conversation threads** with AI agents using the Microsoft Agent Framework.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The ability to suspend and resume conversations is essential for building stateless, scalable applications. This pattern enables:\n",
    "\n",
    "- **Session persistence** across user logins and devices\n",
    "- **Stateless application design** without keeping conversations in memory\n",
    "- **Conversation handoff** between different services or processes\n",
    "- **Long-running conversations** that span hours, days, or weeks\n",
    "- **Backup and recovery** of conversation state\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Thread Types\n",
    "\n",
    "The Agent Framework supports two types of conversation threads:\n",
    "\n",
    "1. **Service-Managed Threads**:\n",
    "   - Conversation history stored in external services (Azure AI, OpenAI)\n",
    "   - Serialization contains only thread ID (~50 bytes)\n",
    "   - Automatically synchronized across instances\n",
    "   - Recommended for production applications\n",
    "\n",
    "2. **In-Memory Threads**:\n",
    "   - Conversation history stored locally in the application\n",
    "   - Serialization contains full message history\n",
    "   - Useful for custom storage backends\n",
    "   - Full control over data storage\n",
    "\n",
    "### Thread Serialization\n",
    "\n",
    "The `serialize()` and `deserialize_thread()` methods enable:\n",
    "- Saving thread state to databases, files, or cache\n",
    "- Restoring conversations with full context\n",
    "- Moving conversations between application instances\n",
    "\n",
    "## 📖 Documentation\n",
    "\n",
    "For more details, see the official documentation:\n",
    "- [Multi-Turn Conversations](https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/multi-turn-conversation?pivots=programming-language-python)\n",
    "- [AgentThread Storage](https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/multi-turn-conversation?pivots=programming-language-python#agentthread-storage)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8b6a8",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **Environment variables configured** in `agent-framework/.env`:\n",
    "   - `OPENAI_API_KEY` (for OpenAI examples)\n",
    "   - Or `AZURE_AI_PROJECT_ENDPOINT` + credentials (for Azure AI examples)\n",
    "\n",
    "2. **Required packages installed**:\n",
    "   ```bash\n",
    "   pip install agent-framework\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084218fa",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc459c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agent_framework.openai import OpenAIChatClient\n",
    "from openai import AsyncAzureOpenAI\n",
    "\n",
    "# Load environment variables\n",
    "env_path = Path(__file__).parent.parent / \".env\" if \"__file__\" in globals() else Path(\"../.env\")\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# Create Azure OpenAI client to be reused\n",
    "azure_client = AsyncAzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\") or os.getenv(\"OPENAI_API_KEY\"),\n",
    "    api_version=\"2024-10-21\",\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "\n",
    "print(\"Environment loaded and Azure OpenAI client created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d34a73",
   "metadata": {},
   "source": [
    "## Example 1: Suspend-Resume with Service-Managed Threads\n",
    "\n",
    "Service-managed threads store conversation history in external services like OpenAI or Azure AI. This is the **recommended pattern for production applications**.\n",
    "\n",
    "### Advantages:\n",
    "- **Lightweight serialization**: Only thread ID is saved (~50 bytes)\n",
    "- **Automatic synchronization**: Multiple app instances share the same conversation\n",
    "- **Scalability**: No need to manage conversation storage\n",
    "- **Cloud backup**: Conversation history is automatically backed up\n",
    "\n",
    "### How It Works:\n",
    "1. Create a thread - service assigns a unique ID\n",
    "2. Have a conversation - messages stored in the service\n",
    "3. Serialize - save only the thread ID\n",
    "4. Deserialize - reconnect to the same thread in the service\n",
    "5. Resume - continue with full conversation context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074475f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def suspend_resume_service_managed_thread():\n",
    "    \"\"\"Demonstrates how to suspend and resume a service-managed thread.\"\"\"\n",
    "    print(\"=== Suspend-Resume Service-Managed Thread ===\")\n",
    "    print()\n",
    "    print(\"📌 Service-managed threads store conversation history in OpenAI/Azure.\")\n",
    "    print(\"   Serialization only contains the thread ID (lightweight).\")\n",
    "    print()\n",
    "\n",
    "    # Create an agent using Azure OpenAI\n",
    "    agent = OpenAIChatClient(\n",
    "        model_id=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
    "        async_client=azure_client\n",
    "    ).create_agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are good at telling jokes.\"\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Agent created\")\n",
    "    print()\n",
    "\n",
    "    # Start a new thread for the agent conversation.\n",
    "    # The underlying service (OpenAI) creates and manages the thread\n",
    "    thread = agent.get_new_thread()\n",
    "\n",
    "    print(\"✓ New thread created (service-managed)\")\n",
    "    print()\n",
    "\n",
    "    # --- Phase 1: Initial Conversation ---\n",
    "    print(\"--- Phase 1: Initial Conversation ---\")\n",
    "    print()\n",
    "    \n",
    "    query = \"Tell me a joke about a pirate.\"\n",
    "    print(f\"User: {query}\")\n",
    "    response = await agent.run(query, thread=thread)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print()\n",
    "\n",
    "    # --- Phase 2: Suspend (Serialize) ---\n",
    "    print(\"--- Phase 2: Suspending Conversation ---\")\n",
    "    print()\n",
    "    \n",
    "    # Serialize the thread state, so it can be stored for later use.\n",
    "    # For service-managed threads, this is very lightweight (just thread ID)\n",
    "    serialized_thread = await thread.serialize()\n",
    "\n",
    "    print(\"📦 Serialized thread:\")\n",
    "    print(f\"   {serialized_thread}\")\n",
    "    print()\n",
    "    print(\"💾 Storage options for serialized thread:\")\n",
    "    print(\"   - Database (SQL, NoSQL)\")\n",
    "    print(\"   - Session storage (cookies, server sessions)\")\n",
    "    print(\"   - Cache (Redis, Memcached)\")\n",
    "    print(\"   - File storage\")\n",
    "    print()\n",
    "    print(f\"💡 Serialized size: ~{len(str(serialized_thread))} bytes\")\n",
    "    print(\"   (In-memory threads contain full message history)\")\n",
    "    print()\n",
    "\n",
    "    # --- Phase 3: Resume (Deserialize) ---\n",
    "    print(\"--- Phase 3: Resuming Conversation ---\")\n",
    "    print()\n",
    "    \n",
    "    # Deserialize the thread state after loading from storage.\n",
    "    # This reconnects to the same thread in the service\n",
    "    resumed_thread = await agent.deserialize_thread(serialized_thread)\n",
    "\n",
    "    print(\"✓ Thread deserialized and reconnected to service\")\n",
    "    print(\"✓ Full conversation history available from service\")\n",
    "    print()\n",
    "\n",
    "    # Continue the conversation - agent has full context from previous messages\n",
    "    query = \"Now tell the same joke in the voice of a pirate, and add some emojis to the joke.\"\n",
    "    print(f\"User: {query}\")\n",
    "    response = await agent.run(query, thread=resumed_thread)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"✅ Conversation successfully resumed with full context!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e458aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the service-managed thread example\n",
    "await suspend_resume_service_managed_thread()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406ced7a",
   "metadata": {},
   "source": [
    "## Example 2: Suspend-Resume with In-Memory Threads\n",
    "\n",
    "In-memory threads store conversation history locally in the application. This pattern provides full control over data storage.\n",
    "\n",
    "### Characteristics:\n",
    "- **Full message history**: Serialization includes all messages\n",
    "- **Larger payload**: Grows with conversation length\n",
    "- **Custom storage**: You control where and how data is stored\n",
    "- **Flexibility**: Works with any storage backend\n",
    "\n",
    "### When to Use:\n",
    "- Custom message store implementations\n",
    "- Specific data residency requirements\n",
    "- Integration with existing data infrastructure\n",
    "- Offline-first applications\n",
    "\n",
    "### How It Works:\n",
    "1. Create a thread - messages stored in memory\n",
    "2. Have a conversation - messages accumulate locally\n",
    "3. Serialize - export all messages as JSON\n",
    "4. Deserialize - restore messages to new thread\n",
    "5. Resume - continue with restored context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1771e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def suspend_resume_in_memory_thread():\n",
    "    \"\"\"Demonstrates how to suspend and resume an in-memory thread.\"\"\"\n",
    "    print(\"=== Suspend-Resume In-Memory Thread ===\")\n",
    "    print()\n",
    "    print(\"📌 In-memory threads store messages locally in the application.\")\n",
    "    print(\"   Serialization contains the full message history.\")\n",
    "    print()\n",
    "\n",
    "    # Create an agent with in-memory threading using Azure OpenAI\n",
    "    agent = OpenAIChatClient(\n",
    "        model_id=os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\", \"gpt-4o\"),\n",
    "        async_client=azure_client\n",
    "    ).create_agent(\n",
    "        name=\"Joker\",\n",
    "        instructions=\"You are good at telling jokes.\"\n",
    "    )\n",
    "\n",
    "    print(\"✓ Agent created\")\n",
    "    print()\n",
    "\n",
    "    # Start a new thread for the agent conversation.\n",
    "    # Messages will be stored in the application's memory\n",
    "    thread = agent.get_new_thread()\n",
    "\n",
    "    print(\"✓ New thread created (in-memory)\")\n",
    "    print()\n",
    "\n",
    "    # --- Phase 1: Initial Conversation ---\n",
    "    print(\"--- Phase 1: Initial Conversation ---\")\n",
    "    print()\n",
    "    \n",
    "    query = \"Tell me a joke about a pirate.\"\n",
    "    print(f\"User: {query}\")\n",
    "    response = await agent.run(query, thread=thread)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print()\n",
    "\n",
    "    # --- Phase 2: Suspend (Serialize) ---\n",
    "    print(\"--- Phase 2: Suspending Conversation ---\")\n",
    "    print()\n",
    "    \n",
    "    # Serialize the thread state, so it can be stored for later use.\n",
    "    # For in-memory threads, this includes the entire message history\n",
    "    serialized_thread = await thread.serialize()\n",
    "\n",
    "    print(\"📦 Serialized thread (first 300 chars):\")\n",
    "    serialized_str = str(serialized_thread)\n",
    "    print(f\"   {serialized_str[:300]}...\")\n",
    "    print()\n",
    "    print(\"💾 This serialized data contains:\")\n",
    "    print(\"   - All user messages\")\n",
    "    print(\"   - All agent responses\")\n",
    "    print(\"   - Message metadata (timestamps, roles, etc.)\")\n",
    "    print()\n",
    "    print(f\"💡 Serialized size: ~{len(serialized_str)} bytes\")\n",
    "    print(\"   (In-memory threads grow with conversation length)\")\n",
    "    print()\n",
    "\n",
    "    # --- Phase 3: Resume (Deserialize) ---\n",
    "    print(\"--- Phase 3: Resuming Conversation ---\")\n",
    "    print()\n",
    "    \n",
    "    # Deserialize the thread state after loading from storage.\n",
    "    # This creates a new in-memory thread with the restored messages\n",
    "    resumed_thread = await agent.deserialize_thread(serialized_thread)\n",
    "\n",
    "    print(\"✓ Thread deserialized with full message history\")\n",
    "    print(\"✓ All previous messages restored to memory\")\n",
    "    print()\n",
    "\n",
    "    # Continue the conversation - agent has access to all previous messages\n",
    "    query = \"Now tell the same joke in the voice of a pirate, and add some emojis to the joke.\"\n",
    "    print(f\"User: {query}\")\n",
    "    response = await agent.run(query, thread=resumed_thread)\n",
    "    print(f\"Agent: {response}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"✅ Conversation successfully resumed with full context!\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd9270c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the in-memory thread example\n",
    "await suspend_resume_in_memory_thread()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e545d75",
   "metadata": {},
   "source": [
    "## Running Both Examples\n",
    "\n",
    "Execute both examples to compare the two threading approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1a7117",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_all_examples():\n",
    "    \"\"\"Run both suspend-resume examples.\"\"\"\n",
    "    print(\"Suspend-Resume Thread Examples\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    await suspend_resume_service_managed_thread()\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    await suspend_resume_in_memory_thread()\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    print(\"✅ All examples completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all examples\n",
    "await run_all_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc4b57",
   "metadata": {},
   "source": [
    "## Comparison: Service-Managed vs In-Memory Threads\n",
    "\n",
    "| Feature | Service-Managed | In-Memory |\n",
    "|---------|----------------|----------|\n",
    "| **Serialization Size** | ~50 bytes (thread ID) | Full message history |\n",
    "| **Storage Location** | External service | Application memory |\n",
    "| **Scalability** | Excellent (shared across instances) | Limited (per-instance) |\n",
    "| **Setup Complexity** | Low (service handles it) | Medium (custom storage) |\n",
    "| **Data Control** | Service-managed | Full control |\n",
    "| **Cost** | Service API calls | Storage costs |\n",
    "| **Best For** | Production apps | Custom requirements |\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### Service-Managed Threads (Recommended)\n",
    "\n",
    "✅ **Use when:**\n",
    "- Building production applications\n",
    "- Need automatic synchronization across instances\n",
    "- Want minimal serialization overhead\n",
    "- Prefer managed infrastructure\n",
    "\n",
    "⚠️ **Consider:**\n",
    "- API costs for storage\n",
    "- Dependency on external service\n",
    "- Data residency in service cloud\n",
    "\n",
    "### In-Memory Threads\n",
    "\n",
    "✅ **Use when:**\n",
    "- Need custom storage backends\n",
    "- Have specific data residency requirements\n",
    "- Want full control over data\n",
    "- Building offline-first applications\n",
    "\n",
    "⚠️ **Consider:**\n",
    "- Larger serialization payloads\n",
    "- Need to implement storage infrastructure\n",
    "- Synchronization across instances\n",
    "\n",
    "## Production Patterns\n",
    "\n",
    "### 1. Database Storage Pattern\n",
    "```python\n",
    "# Serialize and save to database\n",
    "serialized = await thread.serialize()\n",
    "await db.save_conversation(\n",
    "    user_id=user_id,\n",
    "    conversation_id=conversation_id,\n",
    "    thread_data=serialized\n",
    ")\n",
    "\n",
    "# Load and deserialize from database\n",
    "thread_data = await db.load_conversation(conversation_id)\n",
    "thread = await agent.deserialize_thread(thread_data)\n",
    "```\n",
    "\n",
    "### 2. Session Storage Pattern\n",
    "```python\n",
    "# Save to user session\n",
    "session['conversation_thread'] = await thread.serialize()\n",
    "\n",
    "# Restore from session\n",
    "thread = await agent.deserialize_thread(\n",
    "    session['conversation_thread']\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Cache Pattern (Redis)\n",
    "```python\n",
    "# Cache serialized thread\n",
    "serialized = await thread.serialize()\n",
    "await redis.setex(\n",
    "    f\"thread:{thread_id}\",\n",
    "    3600,  # 1 hour TTL\n",
    "    json.dumps(serialized)\n",
    ")\n",
    "\n",
    "# Retrieve from cache\n",
    "cached = await redis.get(f\"thread:{thread_id}\")\n",
    "thread = await agent.deserialize_thread(json.loads(cached))\n",
    "```\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Always serialize before closing threads**: Capture state before disposal\n",
    "2. **Use consistent serialization formats**: JSON is recommended for portability\n",
    "3. **Implement error handling**: Gracefully handle serialization/deserialization failures\n",
    "4. **Set expiration policies**: Clean up old conversations automatically\n",
    "5. **Version your serialization format**: Support format migrations\n",
    "6. **Compress large payloads**: Reduce storage costs for in-memory threads\n",
    "7. **Encrypt sensitive data**: Protect conversation content in storage\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Explore **[Custom Message Stores](2-custom_chat_message_store_thread.ipynb)** for custom backends\n",
    "- Learn about **[Redis Message Store](3-redis_chat_message_store_thread.ipynb)** for distributed scenarios\n",
    "- Review **[Azure AI Thread Serialization](1-azure-ai-thread-serialization.ipynb)** for cloud integration\n",
    "- Read **[Multi-Turn Conversation Documentation](https://learn.microsoft.com/en-us/agent-framework/user-guide/agents/multi-turn-conversation?pivots=programming-language-python)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
