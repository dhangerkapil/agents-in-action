{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcf9bc6f",
   "metadata": {},
   "source": [
    "# Checkpoint with Human-in-the-Loop\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook demonstrates the **checkpoint + human-in-the-loop pattern** - combining workflow persistence with interactive human review. This is essential for production workflows that:\n",
    "\n",
    "1. **Require human approval** at critical decision points\n",
    "2. **Run across sessions** - pause, exit program, resume later\n",
    "3. **Pre-supply responses** - provide answers at restore time\n",
    "4. **Maintain state** across interruptions\n",
    "\n",
    "### Workflow: Release Notes Approval Pipeline\n",
    "\n",
    "```\n",
    "User Brief\n",
    "    ↓\n",
    "BriefPreparer (normalize & prepare)\n",
    "    ↓\n",
    "WriterAgent (draft release notes)\n",
    "    ↓\n",
    "ReviewGateway (request human approval)\n",
    "    ↓\n",
    "┌─────────────────────────────────┐\n",
    "│  RequestInfoExecutor            │\n",
    "│  (pause for human decision)     │\n",
    "│  • \"approve\" → Finalize         │\n",
    "│  • feedback → Revision loop     │\n",
    "└─────────────────────────────────┘\n",
    "    ↓\n",
    "If approved: FinalizeExecutor\n",
    "If feedback: WriterAgent (revise)\n",
    "```\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **FileCheckpointStorage**: Persist workflow state to disk\n",
    "2. **RequestInfoExecutor**: Pause workflow for human approval\n",
    "3. **Checkpoint Summaries**: Inspect pending requests\n",
    "4. **Resume with Responses**: Pre-supply answers at restore\n",
    "5. **Revision Loops**: Iterate based on human feedback\n",
    "6. **State Persistence**: Executor state + shared state\n",
    "\n",
    "### Pause/Resume Flow:\n",
    "\n",
    "1. **Initial Run**: Execute until human approval needed → checkpoint created\n",
    "2. **Program Exit**: Checkpoint saved with \"awaiting human response\" status\n",
    "3. **Later Resume**: Restart program, select checkpoint\n",
    "4. **Pre-supply Response**: Provide decision before resume\n",
    "5. **Auto-apply**: Workflow resumes without re-emitting RequestInfoEvent\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure OpenAI configured with environment variables\n",
    "- Azure CLI authentication: `az login`\n",
    "- Agent Framework installed: `pip install agent-framework`\n",
    "- Write permissions for checkpoint directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b8695",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1d0d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import asyncio\n",
    "from collections.abc import AsyncIterable\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import TYPE_CHECKING, Any\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from agent_framework import (\n",
    "    AgentExecutor,\n",
    "    AgentExecutorRequest,\n",
    "    AgentExecutorResponse,\n",
    "    ChatMessage,\n",
    "    Executor,\n",
    "    FileCheckpointStorage,\n",
    "    RequestInfoEvent,\n",
    "    RequestInfoExecutor,\n",
    "    RequestInfoMessage,\n",
    "    RequestResponse,\n",
    "    Role,\n",
    "    WorkflowBuilder,\n",
    "    WorkflowContext,\n",
    "    WorkflowOutputEvent,\n",
    "    WorkflowRunState,\n",
    "    WorkflowStatusEvent,\n",
    "    handler,\n",
    ")\n",
    "from agent_framework.azure import AzureOpenAIChatClient\n",
    "from azure.identity import AzureCliCredential\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from agent_framework import Workflow\n",
    "    from agent_framework._workflows._checkpoint import WorkflowCheckpoint\n",
    "\n",
    "# Load environment variables from .env file\n",
    "notebook_path = Path().absolute()\n",
    "load_dotenv('../../.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "435a71a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Checking Azure OpenAI environment variables...\n",
      "✅ AZURE_OPENAI_ENDPOINT: https://kd-foundry-project-resource.openai.azure.com/\n",
      "✅ AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "# Verify environment variables are loaded\n",
    "print(\"🔍 Checking Azure OpenAI environment variables...\")\n",
    "endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "deployment_name = os.getenv('AZURE_OPENAI_CHAT_DEPLOYMENT_NAME')\n",
    "\n",
    "if not endpoint or not deployment_name:\n",
    "    raise ValueError(\n",
    "        \"❌ Azure OpenAI environment variables not found.\\n\"\n",
    "        \"Please set AZURE_OPENAI_ENDPOINT and AZURE_OPENAI_CHAT_DEPLOYMENT_NAME in your .env file\"\n",
    "    )\n",
    "\n",
    "print(f\"✅ AZURE_OPENAI_ENDPOINT: {endpoint}\")\n",
    "print(f\"✅ AZURE_OPENAI_CHAT_DEPLOYMENT_NAME: {deployment_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf376fb3",
   "metadata": {},
   "source": [
    "## Configure Checkpoint Directory\n",
    "\n",
    "We create a dedicated directory for this sample to:\n",
    "- Isolate from other samples\n",
    "- Enable clean-up after demonstration\n",
    "- Make checkpoint files easy to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f44781ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory: c:\\Users\\kapildhanger\\OneDrive - Microsoft\\Microsoft_Kapil\\Azure_learning\\agent-framework\\agent-framework\\python\\samples\\getting_started\\workflows\\checkpoint\\notebooks\\tmp\\checkpoints_hitl\n"
     ]
    }
   ],
   "source": [
    "# Create temporary checkpoint directory\n",
    "CHECKPOINT_DIR = Path(\"./tmp/checkpoints_hitl\")\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Checkpoint directory: {CHECKPOINT_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b43ed0",
   "metadata": {},
   "source": [
    "## Define Request/Response Models\n",
    "\n",
    "### HumanApprovalRequest\n",
    "\n",
    "Subclasses `RequestInfoMessage` for typed human approval requests.\n",
    "\n",
    "**Design Principles:**\n",
    "- **Simple primitive types**: Ensures reliable checkpoint serialization\n",
    "- **Context fields**: `draft` and `iteration` provide review context\n",
    "- **Reconstruction-friendly**: Framework can rebuild from JSON\n",
    "\n",
    "**Fields:**\n",
    "- `prompt`: Instructions for human reviewer\n",
    "- `draft`: Current release notes draft\n",
    "- `iteration`: Revision count (shows iteration history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf8bbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HumanApprovalRequest(RequestInfoMessage):\n",
    "    \"\"\"Message sent to the human reviewer via RequestInfoExecutor.\n",
    "    \n",
    "    These fields are intentionally simple because they are serialized into\n",
    "    checkpoints. Keeping them primitive types guarantees the\n",
    "    pending_requests_from_checkpoint helper can reconstruct them on resume.\n",
    "    \"\"\"\n",
    "    prompt: str = \"\"\n",
    "    draft: str = \"\"\n",
    "    iteration: int = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87934fd",
   "metadata": {},
   "source": [
    "## Create Workflow Executors\n",
    "\n",
    "### 1. BriefPreparer\n",
    "\n",
    "**Responsibilities:**\n",
    "- Normalize user brief (collapse whitespace, add period)\n",
    "- Store in shared state for cross-executor visibility\n",
    "- Create deterministic prompt for writer\n",
    "- Kick off agent execution\n",
    "\n",
    "**Why minimal?** Keeping the first executor simple makes checkpoint state easier to reason about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e294ea45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BriefPreparer(Executor):\n",
    "    \"\"\"Normalizes the user brief and sends a single AgentExecutorRequest.\"\"\"\n",
    "\n",
    "    def __init__(self, id: str, agent_id: str) -> None:\n",
    "        super().__init__(id=id)\n",
    "        self._agent_id = agent_id\n",
    "\n",
    "    @handler\n",
    "    async def prepare(self, brief: str, ctx: WorkflowContext[AgentExecutorRequest, str]) -> None:\n",
    "        # Collapse errant whitespace so the prompt is stable between runs\n",
    "        normalized = \" \".join(brief.split()).strip()\n",
    "        if not normalized.endswith(\".\"):\n",
    "            normalized += \".\"\n",
    "        \n",
    "        # Persist the cleaned brief in shared state so downstream executors\n",
    "        # and future checkpoints can recover the original intent\n",
    "        await ctx.set_shared_state(\"brief\", normalized)\n",
    "        \n",
    "        prompt = (\n",
    "            \"You are drafting product release notes. Summarize the brief below in two sentences. \"\n",
    "            \"Keep it positive and end with a call to action.\\n\\n\"\n",
    "            f\"BRIEF: {normalized}\"\n",
    "        )\n",
    "        \n",
    "        # Hand the prompt to the writer agent\n",
    "        await ctx.send_message(\n",
    "            AgentExecutorRequest(messages=[ChatMessage(Role.USER, text=prompt)], should_respond=True),\n",
    "            target_id=self._agent_id,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03690f3e",
   "metadata": {},
   "source": [
    "### 2. ReviewGateway\n",
    "\n",
    "**Responsibilities:**\n",
    "- Route agent drafts to human reviewers\n",
    "- Persist iteration count and draft in executor state\n",
    "- Process human decisions (approve vs. revise)\n",
    "- Loop back to writer with feedback if needed\n",
    "\n",
    "**Key Methods:**\n",
    "\n",
    "#### `on_agent_response()`\n",
    "- Captures agent's draft\n",
    "- Increments iteration counter\n",
    "- Stores state for checkpoint persistence\n",
    "- Sends `HumanApprovalRequest` to `RequestInfoExecutor`\n",
    "\n",
    "#### `on_human_feedback()`\n",
    "- Receives `RequestResponse[HumanApprovalRequest, str]`\n",
    "- If \"approve\": Route to finalizer\n",
    "- If feedback: Send revision request back to writer\n",
    "- Uses `feedback.original_request` for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15a7a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewGateway(Executor):\n",
    "    \"\"\"Routes agent drafts to humans and optionally back for revisions.\"\"\"\n",
    "\n",
    "    def __init__(self, id: str, reviewer_id: str, writer_id: str, finalize_id: str) -> None:\n",
    "        super().__init__(id=id)\n",
    "        self._reviewer_id = reviewer_id\n",
    "        self._writer_id = writer_id\n",
    "        self._finalize_id = finalize_id\n",
    "\n",
    "    @handler\n",
    "    async def on_agent_response(\n",
    "        self,\n",
    "        response: AgentExecutorResponse,\n",
    "        ctx: WorkflowContext[HumanApprovalRequest, str],\n",
    "    ) -> None:\n",
    "        # Capture the agent output for reviewer and checkpoint persistence\n",
    "        draft = response.agent_run_response.text or \"\"\n",
    "        iteration = int((await ctx.get_state() or {}).get(\"iteration\", 0)) + 1\n",
    "        await ctx.set_state({\"iteration\": iteration, \"last_draft\": draft})\n",
    "        \n",
    "        # Emit human approval request - this pauses workflow until answer is supplied\n",
    "        await ctx.send_message(\n",
    "            HumanApprovalRequest(\n",
    "                prompt=\"Review the draft. Reply 'approve' or provide edit instructions.\",\n",
    "                draft=draft,\n",
    "                iteration=iteration,\n",
    "            ),\n",
    "            target_id=self._reviewer_id,\n",
    "        )\n",
    "\n",
    "    @handler\n",
    "    async def on_human_feedback(\n",
    "        self,\n",
    "        feedback: RequestResponse[HumanApprovalRequest, str],\n",
    "        ctx: WorkflowContext[AgentExecutorRequest | str, str],\n",
    "    ) -> None:\n",
    "        # RequestResponse gives us both human data and original request context\n",
    "        reply = (feedback.data or \"\").strip()\n",
    "        state = await ctx.get_state() or {}\n",
    "        draft = state.get(\"last_draft\") or (feedback.original_request.draft if feedback.original_request else \"\")\n",
    "\n",
    "        if reply.lower() == \"approve\":\n",
    "            # Human approval - send to finalizer\n",
    "            await ctx.send_message(draft, target_id=self._finalize_id)\n",
    "            return\n",
    "\n",
    "        # Feedback provided - loop back to writer for revision\n",
    "        guidance = reply or \"Tighten the copy and emphasize customer benefit.\"\n",
    "        iteration = int(state.get(\"iteration\", 1)) + 1\n",
    "        await ctx.set_state({\"iteration\": iteration, \"last_draft\": draft})\n",
    "        \n",
    "        prompt = (\n",
    "            \"Revise the launch note. Respond with the new copy only.\\n\\n\"\n",
    "            f\"Previous draft:\\n{draft}\\n\\n\"\n",
    "            f\"Human guidance: {guidance}\"\n",
    "        )\n",
    "        await ctx.send_message(\n",
    "            AgentExecutorRequest(messages=[ChatMessage(Role.USER, text=prompt)], should_respond=True),\n",
    "            target_id=self._writer_id,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9696f",
   "metadata": {},
   "source": [
    "### 3. FinalizeExecutor\n",
    "\n",
    "**Responsibilities:**\n",
    "- Store approved text in executor state\n",
    "- Yield final output (completes workflow)\n",
    "- Provides observability for diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e5d0e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalizeExecutor(Executor):\n",
    "    \"\"\"Publishes the approved text.\"\"\"\n",
    "\n",
    "    @handler\n",
    "    async def publish(self, text: str, ctx: WorkflowContext[Any, str]) -> None:\n",
    "        # Store output for diagnostics/UI\n",
    "        await ctx.set_state({\"published_text\": text})\n",
    "        # Yield final output - completes workflow\n",
    "        await ctx.yield_output(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1f0cd8",
   "metadata": {},
   "source": [
    "## Build Workflow with Checkpointing\n",
    "\n",
    "### Graph Structure:\n",
    "\n",
    "```\n",
    "BriefPreparer → WriterAgent\n",
    "WriterAgent → ReviewGateway\n",
    "ReviewGateway → RequestInfoExecutor (approval)\n",
    "RequestInfoExecutor → ReviewGateway (human decision)\n",
    "ReviewGateway → WriterAgent (revisions)\n",
    "ReviewGateway → FinalizeExecutor (approval)\n",
    "```\n",
    "\n",
    "### Checkpointing Configuration:\n",
    "\n",
    "- **`.with_checkpointing(checkpoint_storage=...)`**: Enables persistence\n",
    "- **Automatic snapshots**: Created at superstep boundaries\n",
    "- **Identical workflow**: With/without checkpointing uses same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8095efa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Workflow factory created\n"
     ]
    }
   ],
   "source": [
    "def create_workflow(*, checkpoint_storage: FileCheckpointStorage | None = None):\n",
    "    \"\"\"Assemble the workflow graph used by both initial run and resume.\"\"\"\n",
    "\n",
    "    # Create Azure OpenAI agent\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    deployment_name = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\")\n",
    "    chat_client = AzureOpenAIChatClient(\n",
    "        deployment_name=deployment_name,\n",
    "        endpoint=endpoint,\n",
    "        credential=AzureCliCredential()\n",
    "    )\n",
    "    writer = AgentExecutor(\n",
    "        chat_client.create_agent(\n",
    "            instructions=\"Write concise, warm release notes that sound human and helpful.\",\n",
    "        ),\n",
    "        id=\"writer\",\n",
    "    )\n",
    "\n",
    "    # RequestInfoExecutor - lynchpin for human-in-the-loop\n",
    "    review = RequestInfoExecutor(id=\"request_info\")\n",
    "    finalise = FinalizeExecutor(id=\"finalise\")\n",
    "    gateway = ReviewGateway(\n",
    "        id=\"review_gateway\",\n",
    "        reviewer_id=review.id,\n",
    "        writer_id=writer.id,\n",
    "        finalize_id=finalise.id,\n",
    "    )\n",
    "    prepare = BriefPreparer(id=\"prepare_brief\", agent_id=writer.id)\n",
    "\n",
    "    # Build workflow DAG\n",
    "    builder = (\n",
    "        WorkflowBuilder(max_iterations=6)\n",
    "        .set_start_executor(prepare)\n",
    "        .add_edge(prepare, writer)\n",
    "        .add_edge(writer, gateway)\n",
    "        .add_edge(gateway, review)\n",
    "        .add_edge(review, gateway)  # Human resumes loop\n",
    "        .add_edge(gateway, writer)  # Revisions\n",
    "        .add_edge(gateway, finalise)\n",
    "    )\n",
    "\n",
    "    # Opt-in to persistence when caller provides storage\n",
    "    if checkpoint_storage:\n",
    "        builder = builder.with_checkpointing(checkpoint_storage=checkpoint_storage)\n",
    "\n",
    "    return builder.build()\n",
    "\n",
    "print(\"✓ Workflow factory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb575e69",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "### Checkpoint Summary Display\n",
    "\n",
    "Uses framework's `RequestInfoExecutor.checkpoint_summary()` to display:\n",
    "- Checkpoint ID\n",
    "- Iteration count\n",
    "- Target executors\n",
    "- Executor states\n",
    "- Status (\"awaiting human response\", \"completed\")\n",
    "- Draft preview\n",
    "- Pending request IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b8d707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_checkpoint_summary(checkpoints: list) -> None:\n",
    "    \"\"\"Pretty-print saved checkpoints with framework summaries.\"\"\"\n",
    "    print(\"\\nCheckpoint summary:\")\n",
    "    for summary in [\n",
    "        RequestInfoExecutor.checkpoint_summary(cp) \n",
    "        for cp in sorted(checkpoints, key=lambda c: c.timestamp)\n",
    "    ]:\n",
    "        line = (\n",
    "            f\"- {summary.checkpoint_id} | iter={summary.iteration_count} \"\n",
    "            f\"| targets={summary.targets} | states={summary.executor_states}\"\n",
    "        )\n",
    "        if summary.status:\n",
    "            line += f\" | status={summary.status}\"\n",
    "        if summary.draft_preview:\n",
    "            line += f\" | draft_preview={summary.draft_preview}\"\n",
    "        if summary.pending_requests:\n",
    "            line += f\" | pending_request_id={summary.pending_requests[0].request_id}\"\n",
    "        print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b923e59a",
   "metadata": {},
   "source": [
    "### Event Processing\n",
    "\n",
    "Collects events and extracts:\n",
    "- **WorkflowOutputEvent**: Final result\n",
    "- **RequestInfoEvent**: Pending human approvals\n",
    "- **WorkflowStatusEvent**: State transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08da49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_events(events: list[Any]) -> tuple[str | None, list[tuple[str, HumanApprovalRequest]]]:\n",
    "    \"\"\"Echo workflow events to console and collect outstanding requests.\"\"\"\n",
    "    completed_output: str | None = None\n",
    "    requests: list[tuple[str, HumanApprovalRequest]] = []\n",
    "\n",
    "    for event in events:\n",
    "        print(f\"Event: {event}\")\n",
    "        if isinstance(event, WorkflowOutputEvent):\n",
    "            completed_output = event.data\n",
    "        if isinstance(event, RequestInfoEvent) and isinstance(event.data, HumanApprovalRequest):\n",
    "            requests.append((event.request_id, event.data))\n",
    "        elif isinstance(event, WorkflowStatusEvent) and event.state in {\n",
    "            WorkflowRunState.IN_PROGRESS_PENDING_REQUESTS,\n",
    "            WorkflowRunState.IDLE_WITH_PENDING_REQUESTS,\n",
    "        }:\n",
    "            print(f\"Workflow state: {event.state.name}\")\n",
    "\n",
    "    return completed_output, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0409ae",
   "metadata": {},
   "source": [
    "### Interactive Response Collection\n",
    "\n",
    "Prompts user for approval decisions via CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88d950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_for_responses(requests: list[tuple[str, HumanApprovalRequest]]) -> dict[str, str] | None:\n",
    "    \"\"\"Interactive CLI prompt for any live RequestInfo requests.\"\"\"\n",
    "    if not requests:\n",
    "        return None\n",
    "    \n",
    "    answers: dict[str, str] = {}\n",
    "    for request_id, request in requests:\n",
    "        print(\"\\n=== Human approval needed ===\")\n",
    "        print(f\"request_id: {request_id}\")\n",
    "        if request.iteration:\n",
    "            print(f\"Iteration: {request.iteration}\")\n",
    "        print(request.prompt)\n",
    "        print(\"Draft: \\n---\\n\" + request.draft + \"\\n---\")\n",
    "        answer = input(\"Type 'approve' or enter revision guidance (or 'exit' to quit): \").strip()\n",
    "        if answer.lower() == \"exit\":\n",
    "            raise SystemExit(\"Stopped by user.\")\n",
    "        answers[request_id] = answer\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a631f5",
   "metadata": {},
   "source": [
    "### Pre-supply Responses for Resume\n",
    "\n",
    "**Key Feature**: Provide human answers BEFORE resuming checkpoint.\n",
    "\n",
    "**Benefits:**\n",
    "- Workflow resumes immediately with answers\n",
    "- No re-emission of RequestInfoEvent\n",
    "- Ideal for offline approval workflows\n",
    "- Supports batch processing of approvals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080b10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_pre_supply_responses(cp) -> dict[str, str] | None:\n",
    "    \"\"\"Offer to collect responses before resuming a checkpoint.\"\"\"\n",
    "    pending = RequestInfoExecutor.pending_requests_from_checkpoint(cp)\n",
    "    if not pending:\n",
    "        return None\n",
    "\n",
    "    print(\n",
    "        \"This checkpoint still has pending human input. Provide the responses now so the resume step \"\n",
    "        \"applies them immediately and does not re-emit the original RequestInfo event.\"\n",
    "    )\n",
    "    choice = input(\"Pre-supply responses for this checkpoint? [y/N]: \").strip().lower()\n",
    "    if choice not in {\"y\", \"yes\"}:\n",
    "        return None\n",
    "\n",
    "    answers: dict[str, str] = {}\n",
    "    for item in pending:\n",
    "        iteration = item.iteration or 0\n",
    "        print(f\"\\nPending draft (iteration {iteration} | request_id={item.request_id}):\")\n",
    "        draft_text = (item.draft or \"\").strip()\n",
    "        if draft_text:\n",
    "            print(\"Draft:\\n---\\n\" + draft_text + \"\\n---\")\n",
    "        else:\n",
    "            print(\"Draft: [not captured in checkpoint payload - refer to your notes/log]\")\n",
    "        prompt_text = (item.prompt or \"Review the draft\").strip()\n",
    "        print(prompt_text)\n",
    "        answer = input(\"Response ('approve' or guidance, 'exit' to abort): \").strip()\n",
    "        if answer.lower() == \"exit\":\n",
    "            raise SystemExit(\"Resume aborted by user.\")\n",
    "        answers[item.request_id] = answer\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f8b7c",
   "metadata": {},
   "source": [
    "### Stream Consumer Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379d256",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def consume(stream: AsyncIterable[Any]) -> list[Any]:\n",
    "    \"\"\"Materialize an async event stream into a list.\"\"\"\n",
    "    return [event async for event in stream]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305bc88",
   "metadata": {},
   "source": [
    "## Run Interactive Session\n",
    "\n",
    "Executes workflow with human-in-the-loop until:\n",
    "- Completion (WorkflowOutputEvent)\n",
    "- Pause for human input (RequestInfoEvent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fbcd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_interactive_session(workflow, initial_message: str) -> str | None:\n",
    "    \"\"\"Run the workflow until it either finishes or pauses for human input.\"\"\"\n",
    "    pending_responses: dict[str, str] | None = None\n",
    "    completed_output: str | None = None\n",
    "    first = True\n",
    "\n",
    "    while completed_output is None:\n",
    "        if first:\n",
    "            events = await consume(workflow.run_stream(initial_message))\n",
    "            first = False\n",
    "        elif pending_responses:\n",
    "            events = await consume(workflow.send_responses_streaming(pending_responses))\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        completed_output, requests = print_events(events)\n",
    "        if completed_output is None:\n",
    "            pending_responses = prompt_for_responses(requests)\n",
    "\n",
    "    return completed_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f165e30e",
   "metadata": {},
   "source": [
    "## Resume from Checkpoint\n",
    "\n",
    "### Resume Process:\n",
    "\n",
    "1. **Select checkpoint** by ID\n",
    "2. **Optional: Pre-supply responses** for pending requests\n",
    "3. **Call `run_stream_from_checkpoint()`** with checkpoint_id and responses\n",
    "4. **Continue execution** until next pause or completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5808a101",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def resume_from_checkpoint(\n",
    "    workflow,\n",
    "    checkpoint_id: str,\n",
    "    storage: FileCheckpointStorage,\n",
    "    pre_supplied: dict[str, str] | None,\n",
    ") -> None:\n",
    "    \"\"\"Resume a stored checkpoint and continue until completion or another pause.\"\"\"\n",
    "    print(f\"\\nResuming from checkpoint: {checkpoint_id}\")\n",
    "    events = await consume(\n",
    "        workflow.run_stream_from_checkpoint(\n",
    "            checkpoint_id,\n",
    "            checkpoint_storage=storage,\n",
    "            responses=pre_supplied,\n",
    "        )\n",
    "    )\n",
    "    completed_output, requests = print_events(events)\n",
    "    \n",
    "    if pre_supplied and not requests and completed_output is None:\n",
    "        print(\"Pre-supplied responses applied automatically; workflow is now waiting for the next step.\")\n",
    "\n",
    "    pending = prompt_for_responses(requests)\n",
    "    while completed_output is None and pending:\n",
    "        events = await consume(workflow.send_responses_streaming(pending))\n",
    "        completed_output, requests = print_events(events)\n",
    "        if completed_output is None:\n",
    "            pending = prompt_for_responses(requests)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    if completed_output:\n",
    "        print(f\"Workflow completed with: {completed_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07833ef",
   "metadata": {},
   "source": [
    "## Main Execution Flow\n",
    "\n",
    "### Workflow:\n",
    "\n",
    "1. **Clean checkpoint directory**\n",
    "2. **Create workflow with checkpointing**\n",
    "3. **Run initial session** (may pause for approval)\n",
    "4. **List checkpoints** created\n",
    "5. **Optionally resume** from selected checkpoint\n",
    "6. **Pre-supply responses** if checkpoint has pending requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97f4f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main() -> None:\n",
    "    \"\"\"Entry point used by both initial run and subsequent resumes.\"\"\"\n",
    "    \n",
    "    # Clean existing checkpoints for deterministic demo\n",
    "    for file in CHECKPOINT_DIR.glob(\"*.json\"):\n",
    "        file.unlink()\n",
    "\n",
    "    storage = FileCheckpointStorage(storage_path=CHECKPOINT_DIR)\n",
    "    workflow = create_workflow(checkpoint_storage=storage)\n",
    "\n",
    "    brief = (\n",
    "        \"Introduce our limited edition smart coffee grinder. Mention the $249 price, highlight the \"\n",
    "        \"sensor that auto-adjusts the grind, and invite customers to pre-order on the website.\"\n",
    "    )\n",
    "\n",
    "    print(\"Running workflow (human approval required)...\")\n",
    "    completed = await run_interactive_session(workflow, initial_message=brief)\n",
    "    if completed:\n",
    "        print(f\"Initial run completed with final copy: {completed}\")\n",
    "    else:\n",
    "        print(\"Initial run paused for human input.\")\n",
    "\n",
    "    checkpoints = await storage.list_checkpoints()\n",
    "    if not checkpoints:\n",
    "        print(\"No checkpoints recorded.\")\n",
    "        return\n",
    "\n",
    "    render_checkpoint_summary(checkpoints)\n",
    "\n",
    "    sorted_cps = sorted(checkpoints, key=lambda c: c.timestamp)\n",
    "    print(\"\\nAvailable checkpoints:\")\n",
    "    for idx, cp in enumerate(sorted_cps):\n",
    "        print(f\"  [{idx}] id={cp.checkpoint_id} iter={cp.iteration_count}\")\n",
    "\n",
    "    selection = input(\"\\nResume from which checkpoint? (press Enter to skip): \").strip()\n",
    "    if not selection:\n",
    "        print(\"No resume selected. Exiting.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        idx = int(selection)\n",
    "    except ValueError:\n",
    "        print(\"Invalid input; exiting.\")\n",
    "        return\n",
    "\n",
    "    if not 0 <= idx < len(sorted_cps):\n",
    "        print(\"Index out of range; exiting.\")\n",
    "        return\n",
    "\n",
    "    chosen = sorted_cps[idx]\n",
    "    summary = RequestInfoExecutor.checkpoint_summary(chosen)\n",
    "    if summary.status == \"completed\":\n",
    "        print(\"Selected checkpoint already reflects a completed workflow; nothing to resume.\")\n",
    "        return\n",
    "\n",
    "    # Pre-supply responses if desired\n",
    "    pre_responses = maybe_pre_supply_responses(chosen)\n",
    "\n",
    "    resumed_workflow = create_workflow()\n",
    "    await resume_from_checkpoint(resumed_workflow, chosen.checkpoint_id, storage, pre_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05715e1",
   "metadata": {},
   "source": [
    "## Run the Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968bf816",
   "metadata": {},
   "outputs": [],
   "source": [
    "await main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ff134",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### 1. Checkpoint + HITL Pattern\n",
    "\n",
    "**Combination Benefits:**\n",
    "- Workflows survive program restarts\n",
    "- Human decisions preserved across sessions\n",
    "- Offline approval workflows possible\n",
    "- Audit trail via checkpoint files\n",
    "\n",
    "### 2. Pre-supplying Responses\n",
    "\n",
    "**When to use:**\n",
    "- Offline approval collected via email/UI\n",
    "- Batch processing of multiple approvals\n",
    "- Automated testing with mock responses\n",
    "\n",
    "**How it works:**\n",
    "```python\n",
    "# Pre-supply prevents re-emission of RequestInfoEvent\n",
    "responses = {\"request_abc123\": \"approve\"}\n",
    "await workflow.run_stream_from_checkpoint(\n",
    "    checkpoint_id=checkpoint_id,\n",
    "    responses=responses  # Applied immediately on resume\n",
    ")\n",
    "```\n",
    "\n",
    "### 3. Checkpoint Summary Helpers\n",
    "\n",
    "**RequestInfoExecutor.checkpoint_summary()**:\n",
    "- Status: \"awaiting human response\", \"completed\"\n",
    "- Pending requests with IDs\n",
    "- Draft previews\n",
    "- Iteration counts\n",
    "\n",
    "**RequestInfoExecutor.pending_requests_from_checkpoint()**:\n",
    "- Extract pending HumanApprovalRequest objects\n",
    "- Access draft, iteration, prompt fields\n",
    "- Enable pre-supply UI\n",
    "\n",
    "### 4. State Persistence\n",
    "\n",
    "**Executor State:**\n",
    "```python\n",
    "await ctx.set_state({\"iteration\": 2, \"last_draft\": \"...\"})\n",
    "state = await ctx.get_state()\n",
    "```\n",
    "\n",
    "**Shared State:**\n",
    "```python\n",
    "await ctx.set_shared_state(\"brief\", normalized_brief)\n",
    "brief = await ctx.get_shared_state(\"brief\")\n",
    "```\n",
    "\n",
    "Both are persisted in checkpoints!\n",
    "\n",
    "### 5. Production Patterns\n",
    "\n",
    "See comprehensive patterns in notebook for:\n",
    "- Web API integration\n",
    "- Database-backed storage\n",
    "- Email/Slack notifications\n",
    "- Multi-reviewer workflows\n",
    "- Timeout handling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}