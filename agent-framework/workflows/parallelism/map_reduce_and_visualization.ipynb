{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cd0f25e",
   "metadata": {},
   "source": [
    "# Map-Reduce Pattern with Workflow Visualization\n",
    "\n",
    "This notebook demonstrates a complete **map-reduce workflow** implementation with **file-backed intermediate results** and **workflow visualization** capabilities.\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Map-Reduce Architecture\n",
    "- **Split**: Divide input data into chunks for parallel processing\n",
    "- **Map**: Process each chunk independently and emit key-value pairs\n",
    "- **Shuffle**: Group map outputs by key and partition across reducers\n",
    "- **Reduce**: Aggregate values for each key in parallel\n",
    "- **Completion**: Collect final results from all reducers\n",
    "\n",
    "### File-Backed Intermediate Results\n",
    "- **Memory Efficiency**: Store intermediate data in files, not memory\n",
    "- **Scalability**: Handle datasets larger than available RAM\n",
    "- **Persistence**: Intermediate results survive process failures\n",
    "- **Async I/O**: Non-blocking file operations with aiofiles\n",
    "\n",
    "### Shared State Coordination\n",
    "- **Context Sharing**: Executors communicate via `ctx.set_shared_state()`\n",
    "- **Chunk Distribution**: Split executor assigns chunks to mappers\n",
    "- **Partition Routing**: Shuffle executor assigns keys to reducers\n",
    "- **Type-Safe State**: Strongly typed shared state objects\n",
    "\n",
    "### Workflow Visualization\n",
    "- **WorkflowViz**: Generate visual representations of workflow graphs\n",
    "- **Mermaid Diagrams**: Text-based graph rendering\n",
    "- **DiGraph**: NetworkX graph objects for programmatic analysis\n",
    "- **SVG Export**: High-quality vector graphics for documentation\n",
    "\n",
    "## Workflow Architecture\n",
    "\n",
    "```\n",
    "                  Split (divide text into chunks)\n",
    "                    ↓\n",
    "         ┌──────────┼──────────┐\n",
    "      Map_0      Map_1      Map_2   (parallel word counting)\n",
    "         └──────────┼──────────┘\n",
    "                 Shuffle (group by key, partition)\n",
    "                    ↓\n",
    "         ┌──────────┼──────────┬──────────┐\n",
    "     Reduce_0  Reduce_1  Reduce_2  Reduce_3  (parallel aggregation)\n",
    "         └──────────┼──────────┴──────────┘\n",
    "                Completion (collect results)\n",
    "```\n",
    "\n",
    "## What This Example Shows\n",
    "\n",
    "1. **Scalable Parallel Processing**: Map-reduce for word counting\n",
    "2. **File-Backed Storage**: Intermediate results persisted to disk\n",
    "3. **Shared State Coordination**: Dynamic chunk and partition assignment\n",
    "4. **Workflow Visualization**: Generate Mermaid diagrams and SVG exports\n",
    "5. **Marker-Based Phases**: Dataclasses signal workflow progression\n",
    "6. **Configurable Parallelism**: Adjust mapper and reducer counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca111a32",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Import required modules and configure the workflow environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0063a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import asyncio\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import aiofiles\n",
    "\n",
    "from agent_framework.workflows import Executor, ExecutorContext, Workflow\n",
    "from agent_framework.workflows.viz import WorkflowViz\n",
    "\n",
    "\n",
    "load_dotenv('../../.env')\n",
    "# Configuration\n",
    "NUM_MAPPERS = 3  # Number of parallel map executors\n",
    "NUM_REDUCERS = 4  # Number of parallel reduce executors\n",
    "TMP_DIR = Path(\"tmp\")  # Directory for intermediate files\n",
    "RESOURCES_DIR = Path(\"../resources\")  # Input data directory\n",
    "\n",
    "# Ensure tmp directory exists\n",
    "TMP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"✅ Configuration:\")\n",
    "print(f\"   - Mappers: {NUM_MAPPERS}\")\n",
    "print(f\"   - Reducers: {NUM_REDUCERS}\")\n",
    "print(f\"   - Temp directory: {TMP_DIR}\")\n",
    "print(f\"   - Resources directory: {RESOURCES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace971b3",
   "metadata": {},
   "source": [
    "## Define Message Types and Marker Classes\n",
    "\n",
    "We use dataclasses to represent workflow state and phase transitions.\n",
    "\n",
    "### Marker Classes (Phase Signals):\n",
    "- **`SplitCompleted`**: Signals split phase finished, stores chunk assignments\n",
    "- **`MapCompleted`**: Signals map phase finished, stores map result file paths\n",
    "- **`ShuffleCompleted`**: Signals shuffle phase finished, stores shuffle file paths\n",
    "- **`ReduceCompleted`**: Signals reduce phase finished, stores final result files\n",
    "\n",
    "### Shared State:\n",
    "- **`chunk_assignments`**: Maps mapper IDs to text chunks\n",
    "- **`partition_assignments`**: Maps reducer IDs to key ranges\n",
    "- Both stored in workflow context for cross-executor communication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23327548",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SplitCompleted:\n",
    "    \"\"\"Marker indicating split phase completed.\"\"\"\n",
    "    chunk_count: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class MapCompleted:\n",
    "    \"\"\"Marker indicating map phase completed.\"\"\"\n",
    "    file_paths: list[Path]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ShuffleCompleted:\n",
    "    \"\"\"Marker indicating shuffle phase completed.\"\"\"\n",
    "    file_paths: list[Path]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ReduceCompleted:\n",
    "    \"\"\"Marker indicating reduce phase completed.\"\"\"\n",
    "    file_paths: list[Path]\n",
    "\n",
    "\n",
    "print(\"✅ Marker classes defined for workflow phase signaling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f342792",
   "metadata": {},
   "source": [
    "## Split Executor\n",
    "\n",
    "The **SplitExecutor** reads the input text file and divides it into chunks for parallel processing.\n",
    "\n",
    "### Key Features:\n",
    "- Reads input text from `resources/long_text.txt`\n",
    "- Splits text into `NUM_MAPPERS` chunks\n",
    "- Stores chunk assignments in shared state\n",
    "- Each mapper gets a unique chunk via `chunk_assignments` dictionary\n",
    "\n",
    "### Shared State Storage:\n",
    "```python\n",
    "ctx.set_shared_state(\n",
    "    \"chunk_assignments\",\n",
    "    {\"mapper_0\": chunk0, \"mapper_1\": chunk1, ...}\n",
    ")\n",
    "```\n",
    "\n",
    "### Why Shared State?\n",
    "- Mappers need to know which chunk to process\n",
    "- Avoids duplicating large text chunks in memory\n",
    "- Enables dynamic chunk assignment at runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a7c8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitExecutor(Executor[None, SplitCompleted]):\n",
    "    \"\"\"Splits input text into chunks for parallel map processing.\"\"\"\n",
    "\n",
    "    async def execute(self, ctx: ExecutorContext[None]) -> SplitCompleted:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE 1: SPLIT\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Read the input text file\n",
    "        input_file = RESOURCES_DIR / \"long_text.txt\"\n",
    "        async with aiofiles.open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = await f.read()\n",
    "\n",
    "        print(f\"📄 Read {len(text)} characters from {input_file}\")\n",
    "\n",
    "        # Split text into chunks\n",
    "        chunk_size = len(text) // NUM_MAPPERS\n",
    "        chunks = [\n",
    "            text[i * chunk_size : (i + 1) * chunk_size]\n",
    "            for i in range(NUM_MAPPERS - 1)\n",
    "        ]\n",
    "        # Last chunk gets the remainder\n",
    "        chunks.append(text[(NUM_MAPPERS - 1) * chunk_size :])\n",
    "\n",
    "        # Store chunk assignments in shared state\n",
    "        chunk_assignments = {\n",
    "            f\"mapper_{i}\": chunks[i] for i in range(NUM_MAPPERS)\n",
    "        }\n",
    "        ctx.set_shared_state(\"chunk_assignments\", chunk_assignments)\n",
    "\n",
    "        print(f\"✅ Split text into {NUM_MAPPERS} chunks\")\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            print(f\"   - Chunk {i}: {len(chunk)} characters\")\n",
    "\n",
    "        return SplitCompleted(chunk_count=NUM_MAPPERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889293e8",
   "metadata": {},
   "source": [
    "## Map Executor\n",
    "\n",
    "The **MapExecutor** processes assigned chunks and emits word-count pairs.\n",
    "\n",
    "### Key Features:\n",
    "- Each mapper instance processes its assigned chunk\n",
    "- Retrieves chunk from shared state using executor ID\n",
    "- Counts word occurrences in the chunk\n",
    "- Writes `(word, 1)` pairs to file for each word occurrence\n",
    "- Uses async file I/O for non-blocking writes\n",
    "\n",
    "### Output Format:\n",
    "```\n",
    "word1 1\n",
    "word2 1\n",
    "word1 1\n",
    "...\n",
    "```\n",
    "\n",
    "### File-Backed Storage:\n",
    "- Results written to `tmp/map_results_{mapper_id}.txt`\n",
    "- Memory footprint stays constant regardless of chunk size\n",
    "- Enables processing of very large text files\n",
    "\n",
    "### Why Multiple Instances?\n",
    "- Each mapper runs in parallel on its own chunk\n",
    "- Executor ID determines which chunk to process\n",
    "- Fan-out pattern automatically creates instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc8af41",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MapExecutor(Executor[SplitCompleted, MapCompleted]):\n",
    "    \"\"\"Maps words in assigned chunk to (word, 1) pairs.\"\"\"\n",
    "\n",
    "    async def execute(self, ctx: ExecutorContext[SplitCompleted]) -> MapCompleted:\n",
    "        mapper_id = self.id\n",
    "        print(f\"\\n🗺️  MAP [{mapper_id}]: Processing chunk...\")\n",
    "\n",
    "        # Retrieve assigned chunk from shared state\n",
    "        chunk_assignments: dict[str, str] = ctx.get_shared_state(\"chunk_assignments\")\n",
    "        chunk = chunk_assignments[mapper_id]\n",
    "\n",
    "        # Count words\n",
    "        words = chunk.lower().split()\n",
    "        print(f\"   Found {len(words)} words in chunk\")\n",
    "\n",
    "        # Write (word, 1) pairs to file\n",
    "        output_file = TMP_DIR / f\"map_results_{mapper_id}.txt\"\n",
    "        async with aiofiles.open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word in words:\n",
    "                # Clean word (remove punctuation)\n",
    "                word = \"\".join(c for c in word if c.isalnum())\n",
    "                if word:\n",
    "                    await f.write(f\"{word} 1\\n\")\n",
    "\n",
    "        print(f\"   ✅ Wrote results to {output_file}\")\n",
    "        return MapCompleted(file_paths=[output_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf47762",
   "metadata": {},
   "source": [
    "## Shuffle Executor\n",
    "\n",
    "The **ShuffleExecutor** groups map outputs by key and partitions them across reducers.\n",
    "\n",
    "### Key Features:\n",
    "- Reads all map result files\n",
    "- Groups `(word, 1)` pairs by word (key)\n",
    "- Partitions keys across `NUM_REDUCERS` using hash function\n",
    "- Writes grouped data to reducer-specific files\n",
    "- Stores partition assignments in shared state\n",
    "\n",
    "### Partitioning Strategy:\n",
    "```python\n",
    "reducer_id = hash(word) % NUM_REDUCERS\n",
    "```\n",
    "- Ensures same word always goes to same reducer\n",
    "- Distributes keys evenly across reducers\n",
    "- Enables parallel reduce phase\n",
    "\n",
    "### Output Format (per reducer):\n",
    "```\n",
    "word1 1 1 1\n",
    "word2 1 1\n",
    "word3 1\n",
    "...\n",
    "```\n",
    "\n",
    "### Shared State Storage:\n",
    "```python\n",
    "ctx.set_shared_state(\n",
    "    \"partition_assignments\",\n",
    "    {\"reducer_0\": file0, \"reducer_1\": file1, ...}\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a297bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleExecutor(Executor[list[MapCompleted], ShuffleCompleted]):\n",
    "    \"\"\"Shuffles map outputs by grouping by key and partitioning across reducers.\"\"\"\n",
    "\n",
    "    async def execute(\n",
    "        self, ctx: ExecutorContext[list[MapCompleted]]\n",
    "    ) -> ShuffleCompleted:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE 3: SHUFFLE\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        map_results = ctx.get_input_data()\n",
    "\n",
    "        # Collect all map result files\n",
    "        all_map_files = []\n",
    "        for result in map_results:\n",
    "            all_map_files.extend(result.file_paths)\n",
    "\n",
    "        print(f\"📥 Reading {len(all_map_files)} map result files...\")\n",
    "\n",
    "        # Group by key (word) and partition to reducers\n",
    "        partitions: dict[int, dict[str, list[int]]] = {\n",
    "            i: defaultdict(list) for i in range(NUM_REDUCERS)\n",
    "        }\n",
    "\n",
    "        for map_file in all_map_files:\n",
    "            async with aiofiles.open(map_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                async for line in f:\n",
    "                    word, count = line.strip().split()\n",
    "                    # Partition by hash of word\n",
    "                    reducer_id = hash(word) % NUM_REDUCERS\n",
    "                    partitions[reducer_id][word].append(int(count))\n",
    "\n",
    "        # Write partitioned data to reducer-specific files\n",
    "        shuffle_files = []\n",
    "        partition_assignments = {}\n",
    "\n",
    "        for reducer_id, partition in partitions.items():\n",
    "            output_file = TMP_DIR / f\"shuffle_results_{reducer_id}.txt\"\n",
    "            shuffle_files.append(output_file)\n",
    "            partition_assignments[f\"reducer_{reducer_id}\"] = output_file\n",
    "\n",
    "            async with aiofiles.open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                for word, counts in partition.items():\n",
    "                    # Write word and all its counts\n",
    "                    counts_str = \" \".join(str(c) for c in counts)\n",
    "                    await f.write(f\"{word} {counts_str}\\n\")\n",
    "\n",
    "            print(f\"   ✅ Partition {reducer_id}: {len(partition)} unique words → {output_file}\")\n",
    "\n",
    "        # Store partition assignments in shared state\n",
    "        ctx.set_shared_state(\"partition_assignments\", partition_assignments)\n",
    "\n",
    "        print(f\"\\n✅ Shuffle complete: {len(shuffle_files)} partitions created\")\n",
    "        return ShuffleCompleted(file_paths=shuffle_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e7f407",
   "metadata": {},
   "source": [
    "## Reduce Executor\n",
    "\n",
    "The **ReduceExecutor** aggregates counts for each word in its assigned partition.\n",
    "\n",
    "### Key Features:\n",
    "- Each reducer processes its assigned partition file\n",
    "- Retrieves partition file from shared state using executor ID\n",
    "- Sums counts for each word\n",
    "- Writes final `(word, total_count)` pairs to file\n",
    "- Multiple reducers run in parallel\n",
    "\n",
    "### Output Format:\n",
    "```\n",
    "word1 42\n",
    "word2 17\n",
    "word3 8\n",
    "...\n",
    "```\n",
    "\n",
    "### File-Backed Storage:\n",
    "- Results written to `tmp/reduced_results_{reducer_id}.txt`\n",
    "- Each reducer's output is independent\n",
    "- Final word counts distributed across reducer files\n",
    "\n",
    "### Why Multiple Instances?\n",
    "- Each reducer processes different key partitions\n",
    "- Parallel reduce improves performance\n",
    "- No coordination needed between reducers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReduceExecutor(Executor[ShuffleCompleted, ReduceCompleted]):\n",
    "    \"\"\"Reduces word counts by summing all occurrences.\"\"\"\n",
    "\n",
    "    async def execute(self, ctx: ExecutorContext[ShuffleCompleted]) -> ReduceCompleted:\n",
    "        reducer_id = self.id\n",
    "        print(f\"\\n🔄 REDUCE [{reducer_id}]: Aggregating word counts...\")\n",
    "\n",
    "        # Retrieve assigned partition from shared state\n",
    "        partition_assignments: dict[str, Path] = ctx.get_shared_state(\n",
    "            \"partition_assignments\"\n",
    "        )\n",
    "        partition_file = partition_assignments[reducer_id]\n",
    "\n",
    "        # Aggregate counts\n",
    "        word_counts: dict[str, int] = {}\n",
    "        async with aiofiles.open(partition_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            async for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                counts = [int(c) for c in parts[1:]]\n",
    "                word_counts[word] = sum(counts)\n",
    "\n",
    "        print(f\"   Aggregated {len(word_counts)} unique words\")\n",
    "\n",
    "        # Write final results\n",
    "        output_file = TMP_DIR / f\"reduced_results_{reducer_id}.txt\"\n",
    "        async with aiofiles.open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            for word, count in sorted(word_counts.items()):\n",
    "                await f.write(f\"{word} {count}\\n\")\n",
    "\n",
    "        print(f\"   ✅ Wrote results to {output_file}\")\n",
    "        return ReduceCompleted(file_paths=[output_file])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f117fc5d",
   "metadata": {},
   "source": [
    "## Completion Executor\n",
    "\n",
    "The **CompletionExecutor** collects final results from all reducers.\n",
    "\n",
    "### Key Features:\n",
    "- Receives list of all reducer output files\n",
    "- Displays total word count statistics\n",
    "- Shows sample results from each reducer\n",
    "- Confirms successful workflow completion\n",
    "\n",
    "### Final Output:\n",
    "- List of file paths containing word counts\n",
    "- Each file has sorted `(word, count)` pairs\n",
    "- Results distributed across `NUM_REDUCERS` files\n",
    "\n",
    "### Post-Processing Options:\n",
    "- Merge all reducer files for global word counts\n",
    "- Sort by count to find most frequent words\n",
    "- Generate word cloud visualization\n",
    "- Export to database or analytics platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1134c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CompletionExecutor(Executor[list[ReduceCompleted], ReduceCompleted]):\n",
    "    \"\"\"Collects final results from all reducers.\"\"\"\n",
    "\n",
    "    async def execute(\n",
    "        self, ctx: ExecutorContext[list[ReduceCompleted]]\n",
    "    ) -> ReduceCompleted:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PHASE 5: COMPLETION\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        reduce_results = ctx.get_input_data()\n",
    "\n",
    "        # Collect all reducer output files\n",
    "        all_result_files = []\n",
    "        for result in reduce_results:\n",
    "            all_result_files.extend(result.file_paths)\n",
    "\n",
    "        print(f\"\\n📊 Final Results:\")\n",
    "        print(f\"   - Total reducer output files: {len(all_result_files)}\")\n",
    "\n",
    "        # Display sample results from each file\n",
    "        for result_file in all_result_files:\n",
    "            async with aiofiles.open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = await f.readlines()\n",
    "                print(f\"\\n   📄 {result_file.name}: {len(lines)} unique words\")\n",
    "                # Show first 3 words\n",
    "                for line in lines[:3]:\n",
    "                    word, count = line.strip().split()\n",
    "                    print(f\"      - {word}: {count}\")\n",
    "                if len(lines) > 3:\n",
    "                    print(f\"      ... and {len(lines) - 3} more words\")\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"✅ MAP-REDUCE WORKFLOW COMPLETE\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        return ReduceCompleted(file_paths=all_result_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dd3f6d",
   "metadata": {},
   "source": [
    "## Build the Workflow\n",
    "\n",
    "Construct the map-reduce workflow with proper phase connections.\n",
    "\n",
    "### Workflow Construction Steps:\n",
    "\n",
    "1. **Create executor instances** with unique IDs\n",
    "2. **Connect phases** with appropriate edges:\n",
    "   - Split → Map (fan-out to all mappers)\n",
    "   - Map → Shuffle (fan-in from all mappers)\n",
    "   - Shuffle → Reduce (fan-out to all reducers)\n",
    "   - Reduce → Completion (fan-in from all reducers)\n",
    "3. **Set entry point** to Split\n",
    "\n",
    "### Graph Structure:\n",
    "```\n",
    "        Split\n",
    "          ↓ (fan-out)\n",
    "    Map_0, Map_1, Map_2\n",
    "          ↓ (fan-in)\n",
    "       Shuffle\n",
    "          ↓ (fan-out)\n",
    "  Reduce_0, Reduce_1, Reduce_2, Reduce_3\n",
    "          ↓ (fan-in)\n",
    "      Completion\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88cb648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create executor instances\n",
    "split = SplitExecutor()\n",
    "\n",
    "mappers = [MapExecutor(id=f\"mapper_{i}\") for i in range(NUM_MAPPERS)]\n",
    "\n",
    "shuffle = ShuffleExecutor()\n",
    "\n",
    "reducers = [ReduceExecutor(id=f\"reducer_{i}\") for i in range(NUM_REDUCERS)]\n",
    "\n",
    "completion = CompletionExecutor()\n",
    "\n",
    "# Build the workflow\n",
    "workflow = Workflow()\n",
    "\n",
    "# Phase 1 → Phase 2: Split → Map (fan-out)\n",
    "workflow.add_fan_out_edges(split, mappers)\n",
    "\n",
    "# Phase 2 → Phase 3: Map → Shuffle (fan-in)\n",
    "workflow.add_fan_in_edges(mappers, shuffle)\n",
    "\n",
    "# Phase 3 → Phase 4: Shuffle → Reduce (fan-out)\n",
    "workflow.add_fan_out_edges(shuffle, reducers)\n",
    "\n",
    "# Phase 4 → Phase 5: Reduce → Completion (fan-in)\n",
    "workflow.add_fan_in_edges(reducers, completion)\n",
    "\n",
    "# Set entry point\n",
    "workflow.set_entry_point(split)\n",
    "\n",
    "print(\"✅ Map-reduce workflow constructed successfully!\")\n",
    "print(f\"\\nWorkflow phases:\")\n",
    "print(f\"   1. Split (1 executor)\")\n",
    "print(f\"   2. Map ({NUM_MAPPERS} executors in parallel)\")\n",
    "print(f\"   3. Shuffle (1 executor)\")\n",
    "print(f\"   4. Reduce ({NUM_REDUCERS} executors in parallel)\")\n",
    "print(f\"   5. Completion (1 executor)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b446f7",
   "metadata": {},
   "source": [
    "## Workflow Visualization\n",
    "\n",
    "Use **WorkflowViz** to generate visual representations of the workflow graph.\n",
    "\n",
    "### Visualization Options:\n",
    "\n",
    "1. **Mermaid Diagram** (Text-based)\n",
    "   - Renders in documentation and markdown files\n",
    "   - Compatible with GitHub, GitLab, and many IDEs\n",
    "   - Easy to version control\n",
    "\n",
    "2. **DiGraph** (NetworkX)\n",
    "   - Programmatic graph analysis\n",
    "   - Compute graph properties (degree, centrality, etc.)\n",
    "   - Apply graph algorithms\n",
    "\n",
    "3. **SVG Export** (Vector Graphics)\n",
    "   - High-quality documentation images\n",
    "   - Scalable without quality loss\n",
    "   - Embeddable in web pages and PDFs\n",
    "\n",
    "### WorkflowViz API:\n",
    "```python\n",
    "viz = WorkflowViz(workflow)\n",
    "mermaid_code = viz.to_mermaid()  # Generate Mermaid diagram\n",
    "digraph = viz.to_digraph()       # Generate NetworkX DiGraph\n",
    "viz.to_svg(\"workflow.svg\")       # Export as SVG file\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb669f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create WorkflowViz instance\n",
    "viz = WorkflowViz(workflow)\n",
    "\n",
    "# Generate Mermaid diagram\n",
    "mermaid_code = viz.to_mermaid()\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MERMAID DIAGRAM\")\n",
    "print(\"=\"*60)\n",
    "print(mermaid_code)\n",
    "print(\"\\n💡 Copy the above code to https://mermaid.live to visualize\")\n",
    "\n",
    "# Generate NetworkX DiGraph\n",
    "digraph = viz.to_digraph()\n",
    "print(f\"\\n📊 DiGraph Statistics:\")\n",
    "print(f\"   - Nodes: {digraph.number_of_nodes()}\")\n",
    "print(f\"   - Edges: {digraph.number_of_edges()}\")\n",
    "print(f\"   - Node names: {list(digraph.nodes())}\")\n",
    "\n",
    "# Export as SVG (optional - requires graphviz)\n",
    "try:\n",
    "    svg_file = TMP_DIR / \"workflow_graph.svg\"\n",
    "    viz.to_svg(str(svg_file))\n",
    "    print(f\"\\n✅ Workflow graph exported to {svg_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n⚠️  SVG export failed: {e}\")\n",
    "    print(\"   Install graphviz to enable SVG export\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df764212",
   "metadata": {},
   "source": [
    "## Run the Workflow\n",
    "\n",
    "Execute the map-reduce workflow and observe each phase.\n",
    "\n",
    "### Expected Behavior:\n",
    "1. **Split**: Divides text into 3 chunks, stores in shared state\n",
    "2. **Map**: 3 mappers process chunks in parallel, write word pairs to files\n",
    "3. **Shuffle**: Groups words by key, partitions across 4 reducers, writes to files\n",
    "4. **Reduce**: 4 reducers aggregate counts in parallel, write final counts to files\n",
    "5. **Completion**: Displays final statistics and sample results\n",
    "\n",
    "### Performance Notes:\n",
    "- Map and Reduce phases execute in parallel\n",
    "- File I/O is asynchronous (non-blocking)\n",
    "- Memory usage stays constant regardless of input size\n",
    "- Intermediate files enable fault recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0370703f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the workflow\n",
    "result = await workflow.run()\n",
    "\n",
    "print(f\"\\n🎉 Workflow execution complete!\")\n",
    "print(f\"   Final result files: {result.file_paths}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097f32d5",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "Let's examine the final word count results across all reducer files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2a5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate all word counts\n",
    "all_word_counts: dict[str, int] = {}\n",
    "\n",
    "for result_file in result.file_paths:\n",
    "    async with aiofiles.open(result_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        async for line in f:\n",
    "            word, count = line.strip().split()\n",
    "            all_word_counts[word] = int(count)\n",
    "\n",
    "# Sort by count (descending)\n",
    "sorted_words = sorted(all_word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 20 MOST FREQUENT WORDS\")\n",
    "print(\"=\"*60)\n",
    "for i, (word, count) in enumerate(sorted_words[:20], 1):\n",
    "    print(f\"{i:2}. {word:20} → {count:4} occurrences\")\n",
    "\n",
    "print(f\"\\n📊 Total Statistics:\")\n",
    "print(f\"   - Unique words: {len(all_word_counts)}\")\n",
    "print(f\"   - Total word occurrences: {sum(all_word_counts.values())}\")\n",
    "print(f\"   - Average count per word: {sum(all_word_counts.values()) / len(all_word_counts):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c85280",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Map-Reduce Architecture\n",
    "- ✅ **Scalable Pattern**: Processes data too large for single machine\n",
    "- ✅ **Five Phases**: Split → Map → Shuffle → Reduce → Completion\n",
    "- ✅ **Parallel Execution**: Map and Reduce phases run concurrently\n",
    "- ✅ **Data Locality**: Each executor processes independent data chunks\n",
    "\n",
    "### File-Backed Intermediate Results\n",
    "- ✅ **Memory Efficiency**: Constant memory usage regardless of data size\n",
    "- ✅ **Fault Tolerance**: Intermediate files enable recovery from failures\n",
    "- ✅ **Async I/O**: Non-blocking file operations with aiofiles\n",
    "- ✅ **Scalability**: Handle datasets larger than available RAM\n",
    "\n",
    "### Shared State Coordination\n",
    "- ✅ **Type-Safe Communication**: Strongly typed shared state objects\n",
    "- ✅ **Dynamic Assignment**: Runtime chunk and partition allocation\n",
    "- ✅ **Context Isolation**: Each executor has independent context\n",
    "- ✅ **Flexible Coordination**: Supports complex multi-executor patterns\n",
    "\n",
    "### Workflow Visualization\n",
    "- ✅ **Mermaid Diagrams**: Text-based graph rendering for docs\n",
    "- ✅ **DiGraph Objects**: Programmatic graph analysis with NetworkX\n",
    "- ✅ **SVG Export**: High-quality vector graphics for documentation\n",
    "- ✅ **Graph Analytics**: Compute properties and apply algorithms\n",
    "\n",
    "### Marker-Based Phases\n",
    "- ✅ **Type Safety**: Dataclasses ensure type-correct phase transitions\n",
    "- ✅ **Clear Semantics**: Marker classes document workflow stages\n",
    "- ✅ **Metadata Passing**: File paths and counts flow through pipeline\n",
    "- ✅ **Debugging Aid**: Markers make execution flow explicit\n",
    "\n",
    "### Configurable Parallelism\n",
    "- ✅ **Tunable Performance**: Adjust mapper and reducer counts\n",
    "- ✅ **Resource Management**: Balance CPU and I/O utilization\n",
    "- ✅ **Scalability Testing**: Experiment with different configurations\n",
    "- ✅ **Production Optimization**: Profile and tune for workload\n",
    "\n",
    "### When to Use This Pattern\n",
    "- ✅ Processing large text files (logs, documents, web pages)\n",
    "- ✅ Distributed aggregation (analytics, reporting, statistics)\n",
    "- ✅ Data-intensive batch processing\n",
    "- ✅ ETL pipelines with transform and reduce steps\n",
    "- ✅ Any workload with embarrassingly parallel map phase\n",
    "\n",
    "### Best Practices\n",
    "- 🎯 **Partition Wisely**: Balance data distribution across reducers\n",
    "- 🎯 **Use Async I/O**: Prevent blocking on file operations\n",
    "- 🎯 **Clean Intermediate Files**: Remove tmp files after completion\n",
    "- 🎯 **Monitor Progress**: Add logging for phase transitions\n",
    "- 🎯 **Handle Errors**: Implement retry logic for file operations\n",
    "- 🎯 **Tune Parallelism**: Profile to find optimal executor counts\n",
    "- 🎯 **Visualize Workflows**: Generate diagrams for documentation\n",
    "\n",
    "### Performance Optimization\n",
    "- 🚀 Increase mapper count for faster parallel processing\n",
    "- 🚀 Increase reducer count for better aggregation parallelism\n",
    "- 🚀 Use larger chunks for fewer but longer-running map tasks\n",
    "- 🚀 Optimize hash function for better key distribution\n",
    "- 🚀 Use SSDs for faster intermediate file I/O\n",
    "- 🚀 Profile async I/O to identify bottlenecks\n",
    "\n",
    "### Next Steps\n",
    "- Process different file types (CSV, JSON, Parquet)\n",
    "- Implement custom aggregation functions (beyond word count)\n",
    "- Add compression for intermediate files\n",
    "- Integrate with distributed file systems (S3, HDFS)\n",
    "- Build monitoring dashboard for workflow execution\n",
    "- Add checkpointing for long-running workflows\n",
    "- Experiment with different partitioning strategies\n",
    "- Create reusable map-reduce workflow templates"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
